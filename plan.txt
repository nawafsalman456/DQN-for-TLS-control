assume we have 6 traffic light states : [0,5]

assume we need to keep traffic lights order between states (is this right ?):
0->1->2->3->4->5->0->1->...
(may be a big limitation, need to try the algorithms without this assumption).

start with a very simple algorithm:
for every state we can do one of 2 actions:
1- stay in current state
2- go to next state

for example, if we use A* with heuristic function h, and distance function g. (f = g + h)
we have this simple graph : 0->1->2->3->4->5->0
from state i, choose to go to state with the minimal f value:
i : if f(i) < f(i+1)
i+1 : if f(i+1) < f(i)
Note: here g and h are not defined on the above simple graph, they are defined on infinite graph where every state is a pair of (tl_state, timestamp):
(0,0)->(1,1)->(1,2)->(1,3)->(2,4)->...


Q-Learning simple algorithm:
NN:
input: s - vector of (num of waiting vehicles, num of slowing vehicles, num of waiting people, total waiting time, num of buses, and other observations ....)
2-3 hidden layers, each of size 32x32 or 64x64.
output: vector of Q, size = 2:
    Q(s, stay_in_curr_state)
    Q(s, go_to_next_state)
Reward of state s: (minus of) the average time people/vehicles exist in the simulation.
given a state s, pass it through the NN, and pick the action with maximal Q.
Training: use RL to train and calculate the network's parameters.
Later: add GAMMA ? handle noise ? handle unexpected behaviour (add transition probability) ? 

meeting with Ayal:
1- VISUALIZATION - tensorboard : draw gradients,loss, and weights for both policy and target networks
2- Plot total reward For each episode, instead of loss for each step.
3- More info per lane. replace Phase <==> lane
4- Constant time per yellow and red phases (2 sec for yellow and 1 sec for red)

NEXT STEPS for future:
1- add ability to choose a seed - to compare results when running with different parameters on same seed.
2- multi-processing : accelerate the optimization + get access to lab GPUs (write this in the report).
3- save the parameters (or the whole model) with the highest score in another file. so we can have them when program ends. (https://pytorch.org/tutorials/beginner/saving_loading_models.html)
4- Hyper-parameters tuning : using cross-validation ? Later, not in this stage of the project.